import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import os
import glob
import cv2
import torchvision.models.video as models

# =========================================================================
# ‚öôÔ∏è Configuration Section (Ensure filenames match your actual paths)
# =========================================================================
# Model weight paths
SKELETON_MODEL_PATH = "best_model_v3.pth"   # Model trained from train_model_v3.py
RGB_MODEL_PATH = "best_model_rgb.pth"       # Model trained from rgb_model.py

# Data paths
TEST_SKELETON_DIR = "skeleton_data/test"    # Skeleton files generated by batch_extract.py
TEST_VIDEO_DIR = "test_set"                 # Raw video files
TRAIN_CSV = "annotations/train_set_labels.csv" # Used to obtain the label list
OUTPUT_FILE = "test_set_labels_fusion.csv"  # Final submission file

# ‚öñÔ∏è Fusion Weights (Key Strategy: RGB Dominant, Skeleton Complementary)
# RGB (91% Acc) gets higher weight; Skeleton (70% Acc) corrects corner cases (lighting/occlusion)
ALPHA_RGB = 0.8
ALPHA_SKELETON = 0.2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =========================================================================
# üèóÔ∏è 1. Skeleton Model Architecture (Must match train_model_v3.py exactly)
# =========================================================================
class LightweightCNNLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LightweightCNNLSTM, self).__init__()
        
        # 1. Shallow CNN
        self.cnn = nn.Sequential(
            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.1) 
        )
        
        # 2. LSTM
        self.lstm = nn.LSTM(64, hidden_size, num_layers=2, 
                            batch_first=True, bidirectional=True, dropout=0.3)
        
        # 3. Attention
        self.attention = nn.MultiheadAttention(embed_dim=hidden_size*2, num_heads=4, batch_first=True)
        
        # 4. Classifier
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # x: [batch, seq, 99]
        c_in = x.permute(0, 2, 1)  # -> [batch, 99, seq]
        c_out = self.cnn(c_in)     # -> [batch, 64, seq]
        lstm_in = c_out.permute(0, 2, 1)  # -> [batch, seq, 64]
        
        lstm_out, _ = self.lstm(lstm_in)
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        pooled = torch.mean(attn_out, dim=1)
        out = self.fc(pooled)
        return out

# =========================================================================
# üõ†Ô∏è 2. Data Processing Functions (Must match batch_extract.py & rgb_model.py)
# =========================================================================

# --- A. Skeleton Processing (Same logic as train_model_v3.py) ---
def process_skeleton(npy_path):
    FIXED_LENGTH = 100
    
    # If file missing or empty, return zero tensor
    if not os.path.exists(npy_path): return torch.zeros((1, FIXED_LENGTH, 99))
    raw_data = np.load(npy_path)
    if raw_data.shape[0] == 0: return torch.zeros((1, FIXED_LENGTH, 99))

    # 1. Reshape & Slice (132 -> 33*4 -> 33*3)
    # train_model_v3.py uses only (x,y,z), discarding visibility
    frames = raw_data.shape[0]
    data = raw_data.reshape(frames, 33, 4)
    xyz = data[:, :, :3] 
    
    # 2. Root Centering (Hip joint midpoint)
    # 23 = left hip, 24 = right hip
    root = (xyz[:, 23, :] + xyz[:, 24, :]) / 2
    xyz = xyz - root.reshape(frames, 1, 3)
    
    # 3. Shoulder Scaling (Normalize by shoulder width)
    # 11 = left shoulder, 12 = right shoulder
    left_shoulder = xyz[:, 11, :]
    right_shoulder = xyz[:, 12, :]
    dist = np.sqrt(np.sum((left_shoulder - right_shoulder)**2, axis=1))
    dist = np.where(dist < 1e-4, 1.0, dist).reshape(frames, 1, 1)
    xyz_norm = xyz / dist
    
    data = xyz_norm.reshape(frames, 99)

    # 4. Padding / Truncating to fixed length 100
    if data.shape[0] > FIXED_LENGTH:
        start = (data.shape[0] - FIXED_LENGTH) // 2
        data = data[start : start + FIXED_LENGTH, :]
    elif data.shape[0] < FIXED_LENGTH:
        padding = np.zeros((FIXED_LENGTH - data.shape[0], 99))
        data = np.vstack((padding, data))
        
    return torch.FloatTensor(data).unsqueeze(0) # -> [1, 100, 99]

# --- B. RGB Video Processing (Same as rgb_model.py) ---
def process_video(video_path):
    RESIZE_H, RESIZE_W = 128, 128
    NUM_FRAMES = 16
    
    cap = cv2.VideoCapture(video_path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret: break
            # Must match training: Resize -> BGR to RGB
            frame = cv2.resize(frame, (RESIZE_W, RESIZE_H))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
    finally:
        cap.release()
        
    if len(frames) == 0:
        return torch.zeros((1, 3, NUM_FRAMES, RESIZE_H, RESIZE_W))

    # 1. Uniform sampling of 16 frames
    indices = np.linspace(0, len(frames) - 1, NUM_FRAMES).astype(int)
    sampled_frames = np.array([frames[i] for i in indices])
    
    # 2. Convert to tensor & normalize (Kinetics-400 parameters)
    buffer = torch.FloatTensor(sampled_frames).permute(3, 0, 1, 2) / 255.0
    
    # Manual normalization (same as rgb_model.py patched version)
    mean = torch.tensor([0.432, 0.394, 0.376]).view(3, 1, 1, 1)
    std = torch.tensor([0.228, 0.221, 0.217]).view(3, 1, 1, 1)
    buffer = (buffer - mean) / std
    
    return buffer.unsqueeze(0)  # -> [1, 3, 16, 128, 128]

# =========================================================================
# üöÄ 3. Main Execution Logic
# =========================================================================
if __name__ == "__main__":
    print(f"üöÄ Starting Two-Stream Fusion Inference | Device: {device}")
    
    # 1. Load label mapping
    df = pd.read_csv(TRAIN_CSV, header=None)
    unique_labels = sorted(df.iloc[:, 1].unique())
    int_to_label = {i: name for i, name in enumerate(unique_labels)}
    num_classes = len(unique_labels)
    print(f"üìã Labels loaded: {num_classes} classes")

    # 2. Load skeleton model
    print(f"üß† Loading Skeleton Model: {SKELETON_MODEL_PATH}")
    if not os.path.exists(SKELETON_MODEL_PATH):
        print(f"‚ùå Error: Skeleton model not found: {SKELETON_MODEL_PATH}")
        exit()
    skel_model = LightweightCNNLSTM(input_size=99, hidden_size=128, num_classes=num_classes).to(device)
    skel_model.load_state_dict(torch.load(SKELETON_MODEL_PATH, map_location=device))
    skel_model.eval()

    # 3. Load RGB model
    print(f"üß† Loading RGB Model: {RGB_MODEL_PATH}")
    if not os.path.exists(RGB_MODEL_PATH):
        print(f"‚ùå Error: RGB model not found: {RGB_MODEL_PATH}")
        exit()
    rgb_model = models.r2plus1d_18(weights=None)  # No pretrained weights needed during inference
    rgb_model.fc = nn.Linear(rgb_model.fc.in_features, num_classes)
    rgb_model.load_state_dict(torch.load(RGB_MODEL_PATH, map_location=device))
    rgb_model.to(device)
    rgb_model.eval()

    # 4. Get test file list
    # Prefer .avi, fallback to .mp4
    test_files = glob.glob(os.path.join(TEST_VIDEO_DIR, "*.avi"))
    if len(test_files) == 0: 
        test_files = glob.glob(os.path.join(TEST_VIDEO_DIR, "*.mp4"))
    
    print(f"üî• Processing {len(test_files)} test samples...")
    print(f"‚öñÔ∏è Fusion Strategy: RGB={ALPHA_RGB} + Skeleton={ALPHA_SKELETON}")
    
    results = []
    
    with torch.no_grad():
        for i, video_path in enumerate(test_files):
            file_id = os.path.splitext(os.path.basename(video_path))[0]
            video_name = file_id + ".avi"  # Required submission format
            
            # Skeleton file path
            npy_path = os.path.join(TEST_SKELETON_DIR, file_id + ".npy")
            
            # --- Stream 1: Skeleton Inference ---
            skel_input = process_skeleton(npy_path).to(device)
            skel_logits = skel_model(skel_input)
            skel_probs = torch.softmax(skel_logits, dim=1)

            # --- Stream 2: RGB Inference ---
            rgb_input = process_video(video_path).to(device)
            rgb_logits = rgb_model(rgb_input)
            rgb_probs = torch.softmax(rgb_logits, dim=1)
            
            # --- Late Fusion ---
            # Formula: Final_Prob = w1 * P_rgb + w2 * P_skel
            final_probs = (ALPHA_RGB * rgb_probs) + (ALPHA_SKELETON * skel_probs)
            
            # Select class with highest prob
            _, predicted = torch.max(final_probs, 1)
            label_name = int_to_label[predicted.item()]
            
            results.append([video_name, label_name])
            
            if (i+1) % 50 == 0:
                print(f"  Processed {i+1}/{len(test_files)}")

    # 5. Save results to CSV
    out_df = pd.DataFrame(results)
    out_df.to_csv(OUTPUT_FILE, index=False, header=False)
    
    print(f"\nüéâ Fusion inference complete!")
    print(f"üìÑ Submission file generated: {os.path.abspath(OUTPUT_FILE)}")
    print("üí° Tip: Describe this 'Two-Stream Architecture' clearly in the report for Originality points.")
