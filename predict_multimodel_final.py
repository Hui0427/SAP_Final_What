import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import os
import glob
import cv2
import torchvision.models.video as models

# =========================================================================
# âš™ï¸ é…ç½®åŒºåŸŸ (è¯·ç¡®è®¤æ–‡ä»¶åæ˜¯å¦ä¸ä½ å®é™…ä¿å­˜çš„ä¸€è‡´)
# =========================================================================
# æ¨¡å‹æƒé‡è·¯å¾„
SKELETON_MODEL_PATH = "best_model_v3.pth"   # å¯¹åº” train_model_v3.py è®­ç»ƒå‡ºçš„æ¨¡å‹
RGB_MODEL_PATH = "best_model_rgb.pth"       # å¯¹åº” rgb_model.py è®­ç»ƒå‡ºçš„æ¨¡å‹

# æ•°æ®è·¯å¾„
TEST_SKELETON_DIR = "skeleton_data/test"    # batch_extract.py ç”Ÿæˆçš„éª¨æ¶æ–‡ä»¶å¤¹
TEST_VIDEO_DIR = "test_set"                 # åŸå§‹è§†é¢‘æ–‡ä»¶å¤¹
TRAIN_CSV = "annotations/train_set_labels.csv" # ç”¨äºè·å–æ ‡ç­¾åˆ—è¡¨
OUTPUT_FILE = "test_set_labels_fusion.csv"  # æœ€ç»ˆæäº¤æ–‡ä»¶

# âš–ï¸ èåˆæƒé‡ (å…³é”®ç­–ç•¥ï¼šRGBä¸ºä¸»ï¼Œéª¨æ¶ä¸ºè¾…)
# RGB (91% Acc) æƒé‡ç»™é«˜ç‚¹ï¼›éª¨æ¶ (70% Acc) è´Ÿè´£ä¿®æ­£å…‰ç…§/é®æŒ¡å¸¦æ¥çš„Corner Case
ALPHA_RGB = 0.8
ALPHA_SKELETON = 0.2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =========================================================================
# ğŸ—ï¸ 1. éª¨æ¶æ¨¡å‹æ¶æ„ (å¿…é¡»å®Œå…¨å¤åˆ» train_model_v3.py)
# =========================================================================
class LightweightCNNLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LightweightCNNLSTM, self).__init__()
        
        # 1. æµ…å±‚ CNN
        self.cnn = nn.Sequential(
            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.1) 
        )
        
        # 2. LSTM
        self.lstm = nn.LSTM(64, hidden_size, num_layers=2, 
                            batch_first=True, bidirectional=True, dropout=0.3)
        
        # 3. Attention
        self.attention = nn.MultiheadAttention(embed_dim=hidden_size*2, num_heads=4, batch_first=True)
        
        # 4. Classifier
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # x: [batch, seq, 99]
        c_in = x.permute(0, 2, 1) # -> [batch, 99, seq]
        c_out = self.cnn(c_in)    # -> [batch, 64, seq]
        lstm_in = c_out.permute(0, 2, 1) # -> [batch, seq, 64]
        
        lstm_out, _ = self.lstm(lstm_in)
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        pooled = torch.mean(attn_out, dim=1)
        out = self.fc(pooled)
        return out

# =========================================================================
# ğŸ› ï¸ 2. æ•°æ®å¤„ç†å‡½æ•° (ä¸¥æ ¼å¯¹åº” batch_extract.py å’Œ rgb_model.py)
# =========================================================================

# --- A. éª¨æ¶å¤„ç† (train_model_v3.py çš„é€»è¾‘) ---
def process_skeleton(npy_path):
    FIXED_LENGTH = 100
    
    # å®¹é”™å¤„ç†ï¼šå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè¿”å›å…¨0å¼ é‡
    if not os.path.exists(npy_path): return torch.zeros((1, FIXED_LENGTH, 99))
    raw_data = np.load(npy_path)
    if raw_data.shape[0] == 0: return torch.zeros((1, FIXED_LENGTH, 99))

    # 1. Reshape & Slice (132 -> 33*4 -> 33*3)
    # train_model_v3.py åªå–äº†å‰3ç»´ (x,y,z)ï¼Œä¸¢å¼ƒäº† visibility
    frames = raw_data.shape[0]
    data = raw_data.reshape(frames, 33, 4)
    xyz = data[:, :, :3] 
    
    # 2. Root Centering (é«‹å…³èŠ‚ä¸­å¿ƒåŒ–)
    # 23=å·¦é«‹, 24=å³é«‹
    root = (xyz[:, 23, :] + xyz[:, 24, :]) / 2
    xyz = xyz - root.reshape(frames, 1, 3)
    
    # 3. Shoulder Scaling (è‚©å®½å½’ä¸€åŒ–)
    # 11=å·¦è‚©, 12=å³è‚©
    left_shoulder = xyz[:, 11, :]
    right_shoulder = xyz[:, 12, :]
    dist = np.sqrt(np.sum((left_shoulder - right_shoulder)**2, axis=1))
    dist = np.where(dist < 1e-4, 1.0, dist).reshape(frames, 1, 1)
    xyz_norm = xyz / dist
    
    data = xyz_norm.reshape(frames, 99)

    # 4. Padding / Truncating (å›ºå®šé•¿åº¦ 100)
    if data.shape[0] > FIXED_LENGTH:
        start = (data.shape[0] - FIXED_LENGTH) // 2
        data = data[start : start + FIXED_LENGTH, :]
    elif data.shape[0] < FIXED_LENGTH:
        padding = np.zeros((FIXED_LENGTH - data.shape[0], 99))
        data = np.vstack((padding, data))
        
    return torch.FloatTensor(data).unsqueeze(0) # å¢åŠ  batch ç»´åº¦ -> [1, 100, 99]

# --- B. RGB è§†é¢‘å¤„ç† (rgb_model.py çš„é€»è¾‘) ---
def process_video(video_path):
    RESIZE_H, RESIZE_W = 128, 128
    NUM_FRAMES = 16
    
    cap = cv2.VideoCapture(video_path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret: break
            # å¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼šResize -> BGRè½¬RGB
            frame = cv2.resize(frame, (RESIZE_W, RESIZE_H))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
    finally:
        cap.release()
        
    if len(frames) == 0:
        return torch.zeros((1, 3, NUM_FRAMES, RESIZE_H, RESIZE_W))

    # 1. å‡åŒ€é‡‡æ · 16 å¸§
    indices = np.linspace(0, len(frames) - 1, NUM_FRAMES).astype(int)
    sampled_frames = np.array([frames[i] for i in indices])
    
    # 2. è½¬ Tensor & å½’ä¸€åŒ– (Kinetics-400 å‚æ•°)
    buffer = torch.FloatTensor(sampled_frames).permute(3, 0, 1, 2) / 255.0
    
    # æ‰‹åŠ¨å½’ä¸€åŒ– (å¯¹åº” rgb_model.py ä¸­çš„ä¿®å¤ä»£ç )
    mean = torch.tensor([0.432, 0.394, 0.376]).view(3, 1, 1, 1)
    std = torch.tensor([0.228, 0.221, 0.217]).view(3, 1, 1, 1)
    buffer = (buffer - mean) / std
    
    return buffer.unsqueeze(0) # å¢åŠ  batch ç»´åº¦ -> [1, 3, 16, 128, 128]

# =========================================================================
# ğŸš€ 3. ä¸»æ‰§è¡Œé€»è¾‘
# =========================================================================
if __name__ == "__main__":
    print(f"ğŸš€ å¯åŠ¨å¤šæ¨¡æ€èåˆæ¨ç† (Two-Stream Fusion) | è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ ‡ç­¾æ˜ å°„
    df = pd.read_csv(TRAIN_CSV, header=None)
    unique_labels = sorted(df.iloc[:, 1].unique())
    int_to_label = {i: name for i, name in enumerate(unique_labels)}
    num_classes = len(unique_labels)
    print(f"ğŸ“‹ æ ‡ç­¾åŠ è½½å®Œæ¯•: {num_classes} ç±»")

    # 2. åŠ è½½éª¨æ¶æ¨¡å‹
    print(f"ğŸ§  åŠ è½½éª¨æ¶æ¨¡å‹: {SKELETON_MODEL_PATH}")
    if not os.path.exists(SKELETON_MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°éª¨æ¶æ¨¡å‹æ–‡ä»¶ {SKELETON_MODEL_PATH}")
        exit()
    # éª¨æ¶æ¨¡å‹å‚æ•°å¿…é¡»ä¸ train_model_v3.py ä¸€è‡´: input=99, hidden=128
    skel_model = LightweightCNNLSTM(input_size=99, hidden_size=128, num_classes=num_classes).to(device)
    skel_model.load_state_dict(torch.load(SKELETON_MODEL_PATH, map_location=device))
    skel_model.eval()

    # 3. åŠ è½½ RGB æ¨¡å‹
    print(f"ğŸ§  åŠ è½½ RGB æ¨¡å‹: {RGB_MODEL_PATH}")
    if not os.path.exists(RGB_MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° RGB æ¨¡å‹æ–‡ä»¶ {RGB_MODEL_PATH}")
        exit()
    # RGB æ¨¡å‹æ¶æ„å¿…é¡»ä¸ rgb_model.py ä¸€è‡´: r2plus1d_18
    rgb_model = models.r2plus1d_18(weights=None) # æ¨ç†æ—¶ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡
    rgb_model.fc = nn.Linear(rgb_model.fc.in_features, num_classes)
    rgb_model.load_state_dict(torch.load(RGB_MODEL_PATH, map_location=device))
    rgb_model.to(device)
    rgb_model.eval()

    # 4. è·å–æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    # ä¼˜å…ˆæ‰¾ .aviï¼Œæ‰¾ä¸åˆ°å†æ‰¾ .mp4
    test_files = glob.glob(os.path.join(TEST_VIDEO_DIR, "*.avi"))
    if len(test_files) == 0: 
        test_files = glob.glob(os.path.join(TEST_VIDEO_DIR, "*.mp4"))
    
    print(f"ğŸ”¥ å¼€å§‹å¤„ç† {len(test_files)} ä¸ªæµ‹è¯•æ ·æœ¬...")
    print(f"âš–ï¸ èåˆç­–ç•¥: RGBæƒé‡ {ALPHA_RGB} + Skeletonæƒé‡ {ALPHA_SKELETON}")
    
    results = []
    
    with torch.no_grad():
        for i, video_path in enumerate(test_files):
            file_id = os.path.splitext(os.path.basename(video_path))[0]
            video_name = file_id + ".avi" # æäº¤æ ¼å¼è¦æ±‚ä¿æŒ avi åç¼€
            
            # å¯¹åº”çš„éª¨æ¶æ–‡ä»¶è·¯å¾„
            npy_path = os.path.join(TEST_SKELETON_DIR, file_id + ".npy")
            
            # --- Stream 1: Skeleton æ¨ç† ---
            skel_input = process_skeleton(npy_path).to(device)
            skel_logits = skel_model(skel_input)
            skel_probs = torch.softmax(skel_logits, dim=1) # Logits -> æ¦‚ç‡åˆ†å¸ƒ
            
            # --- Stream 2: RGB æ¨ç† ---
            rgb_input = process_video(video_path).to(device)
            rgb_logits = rgb_model(rgb_input)
            rgb_probs = torch.softmax(rgb_logits, dim=1)   # Logits -> æ¦‚ç‡åˆ†å¸ƒ
            
            # --- Late Fusion (åŠ æƒèåˆ) ---
            # æ ¸å¿ƒå…¬å¼: Final_Prob = w1 * P_rgb + w2 * P_skel
            final_probs = (ALPHA_RGB * rgb_probs) + (ALPHA_SKELETON * skel_probs)
            
            # å–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœ
            _, predicted = torch.max(final_probs, 1)
            label_name = int_to_label[predicted.item()]
            
            results.append([video_name, label_name])
            
            if (i+1) % 50 == 0: print(f"  å·²å¤„ç† {i+1}/{len(test_files)}")

    # 5. ä¿å­˜ç»“æœåˆ° CSV
    out_df = pd.DataFrame(results)
    # æ ¹æ®ä½œä¸šè¦æ±‚ï¼Œé€šå¸¸ä¸éœ€è¦ headerï¼Œindex ä¹Ÿä¸è¦
    out_df.to_csv(OUTPUT_FILE, index=False, header=False)
    
    print(f"\nğŸ‰ èåˆæ¨ç†å®Œæˆï¼")
    print(f"ğŸ“„ æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {os.path.abspath(OUTPUT_FILE)}")
    print("ğŸ’¡ æç¤º: è¯·åœ¨æŠ¥å‘Šä¸­è¯¦ç»†æè¿°è¿™ä¸ª 'Two-Stream Architecture' ä»¥è·å¾— Originality åŠ åˆ†ã€‚")